{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# Initialize empty list to store features and corresponding labels\n",
    "mfcc_features = []\n",
    "labels = []\n",
    "# Manual genre mapping\n",
    "genre_mapping = {'blues':0, 'classical':1, 'country':2, 'disco':3, 'hiphop':4, 'jazz':5, 'metal':6, 'pop':7, 'reggae':8, 'rock':9}\n",
    "# Initialize path of the dataset, sample ratio, segement duration and overlap\n",
    "dataset_path = \"/content/drive/MyDrive/Data\"\n",
    "segment_duration = 5\n",
    "overlap = 0.5\n",
    "SR = 22050\n",
    "# SR = 44100\n",
    "n_fft=2048\n",
    "hop_length=512\n",
    "\n",
    "# Iterate through each genre folder\n",
    "for genre in os.listdir(dataset_path):\n",
    "    genre_path = os.path.join(dataset_path, genre)\n",
    "\n",
    "    # Iterate through each audio file in the genre folder\n",
    "    for filename in os.listdir(genre_path):\n",
    "       music_path = os.path.join(genre_path, filename)\n",
    "       try:\n",
    "           # Load the audio file\n",
    "           y, sr = librosa.load(music_path, sr=SR)\n",
    "\n",
    "           # Calculate the number of samples per segment\n",
    "           segment_samples = int(segment_duration * sr)\n",
    "\n",
    "           # Calculate the number of samples to overlap\n",
    "           overlap_samples = int(overlap * segment_samples)\n",
    "\n",
    "           # Extract MFCC features for each segment\n",
    "           for i in range(0, len(y) - segment_samples + 1, segment_samples - overlap_samples):\n",
    "                segment = y[i:i + segment_samples]\n",
    "\n",
    "                # Extract MFCC features\n",
    "                mfcc = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13,n_fft=n_fft, hop_length=hop_length)\n",
    "                mfcc = mfcc.T\n",
    "\n",
    "                # Append the MFCC features and corresponding label\n",
    "                mfcc_features.append(mfcc)\n",
    "                labels.append(genre_mapping[genre])\n",
    "\n",
    "       except Exception as e:\n",
    "            print(f\"Error processing {music_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for pitch shifting\n",
    "def pitch_shift(mfcc, sr, n_steps=2):\n",
    "    return librosa.effects.pitch_shift(y=mfcc, sr=sr, n_steps=n_steps)\n",
    "\n",
    "# Data augmentation parameters\n",
    "pitch_shift_steps = 2\n",
    "# Data augmentation after collecting features and labels\n",
    "augmented_mfcc_features = []\n",
    "augmented_labels = []\n",
    "\n",
    "for mfcc, label in zip(mfcc_features, labels):\n",
    "    augmented_mfcc = pitch_shift(mfcc.flatten(), SR, n_steps=pitch_shift_steps).reshape(mfcc.T.shape).T\n",
    "    augmented_mfcc_features.append(augmented_mfcc)\n",
    "    augmented_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the original features and labels with augmented ones\n",
    "mfcc_features.extend(augmented_mfcc_features)\n",
    "labels.extend(augmented_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to numpy array\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and normalize the mfcc features\n",
    "mfcc1_features = [mfcc.reshape((mfcc.shape[0], mfcc.shape[1], 1)) for mfcc in mfcc_features]\n",
    "mfcc1_features = np.array(mfcc1_features)\n",
    "mfcc1_features = (mfcc1_features - np.mean(mfcc1_features)) / np.std(mfcc1_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the folder path\n",
    "folder_path = '/content/drive/MyDrive/Models'\n",
    "\n",
    "# Ensure the folder exists, create it if necessary\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Save the NumPy array to the folder\n",
    "np.save(os.path.join(folder_path, 'mfcc1_features.npy'), mfcc1_features)\n",
    "np.save(os.path.join(folder_path, 'labels.npy'), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(mfcc1_features, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "y_train_onehot = to_categorical(y_train,num_classes=10)\n",
    "y_val_onehot = to_categorical(y_val,num_classes=10)\n",
    "y_test_onehot = to_categorical(y_test,num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation along the axis of features\n",
    "mean_values_normalized = [float(np.mean(mfcc1_features))]\n",
    "std_deviations_normalized = [float(np.std(mfcc1_features))]\n",
    "\n",
    "# Print the mean and standard deviation for each feature\n",
    "for i, (mean, std) in enumerate(zip(mean_values_normalized, std_deviations_normalized), 3):\n",
    "    print(f\"Feature {i}: Mean = {mean:.3f}, Standard Deviation = {std:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "model = Sequential()\n",
    "\n",
    "# 1st conv layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd conv layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd conv layer\n",
    "model.add(Conv2D(256, (2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# flatten output and feed it into dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# output layer\n",
    "num_classes = len(np.unique(y_train_onehot, axis=0))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "# train model\n",
    "history = model.fit(X_train, y_train_onehot, validation_data=(X_val, y_val_onehot), batch_size=32, epochs=50, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_onehot)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
